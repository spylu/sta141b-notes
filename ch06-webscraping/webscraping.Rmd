---
title: "Web Scrpping"
output:
  html_document:
    df_print: paged
  pdf_document: default
date: "02-25-2020"
---

```{r}
library(tidyverse)
# Similar to beautifulsoup but younger
library(rvest)
```

# HTML and XML

Here is an example of a simple HTML page:

Each `<>` thing signifies tag
```html
<!DOCTYPE html>
<html>
<head>
<title>Page Title</title>
</head>
<body>
<h1>This is a Heading</h1>
<p>This is a paragraph.</p>
</body>
</html>
```

Of course, there are a lot of different tags. Go to https://www.w3schools.com/html/ to see some html basics.
Nevertheless, all these tags are predefined so your browser knows what each tag means.

(We only need to only a bit HTML in order to do web scrapping)


## Look at web page source code

It is important to identify the thing that you want to scrape in the webpage. The best way to do it is to use the inspect function in the Chrome browser.

The inspect feature pops out a whole pane of the browser and lets you poke into each text block to see how it's constructed.

## CSS selector

CSS is used to control webpage style. CSS selector is the most common way to locate an object in a website. 

`Tag` and `class` are not unique.

- `tag`: the name of the tag, for example `td` and `a` tags
- `.class`: class of an object
- `#id`: id of an object


## imdb example

Suppose we want to get the list of most top rated movies from https://www.imdb.com/chart/top/?ref_=nv_mv_250

We see that all movies names are under the `<td>/<a>` nodes. The `<td>` nodes have class name `titleColumn`.

```{r}
html <- read_html("https://www.imdb.com/chart/top/?ref_=nv_mv_250")
# it finds all the <td> nodes, but we only need the node with class `titleColumn`
td_nodes <- html %>% html_nodes("td")
# it finds all the <td> nodes with class titleColumn
title_columns <- html %>% html_nodes("td.titleColumn")
# it finds all the <a> nodes within <td> nodes with class titleColumn
a_nodes <- title_columns %>% html_nodes("a")
# html_text to get the values inside the <a> </a> tag
movie_names <- a_nodes %>% html_text()
head(movie_names)
```

Put everything together
```{r}
movie_names <- html %>%
  html_nodes("td.titleColumn") %>% 
# You can combine "a" with above by putting it behind "td.titleColumn" but it's recommended not to
  html_nodes("a") %>%
  html_text()
```


Now, we also want to capture the ratings.

```{r}
imdb_ratings <-  html %>%
# Don't put space behind period because that would indicate descendant
  html_nodes("td.ratingColumn.imdbRating") %>%
# Only extracting the "strong" text, i.e., the ratings
  html_nodes("strong") %>%
  html_text()
```

```{r}
tibble(title = movie_names, rating = imdb_ratings)
```

How if you also want to get the years? There is also a cute function `html_table`.

```{r}
# using `html_nodes` would get you list of things
html %>% html_node("table.chart.full-width") %>%
  html_table() %>%
  # Makes sure names are unique so you can `select` later
  as_tibble(.name_repair = "unique") %>%
  select(rank_and_title = `Rank & Title`, rating = `IMDb Rating`) %>%
  # Need `separate` because rank and title are merged
  separate(rank_and_title, c("rank", "title", "year"), sep = "\n")
```


Now, we want to url link to the movie "The Shawshank Redemption" to scrape stuff from that webpage.

```{r}
shawshank_url <- html %>%
  html_nodes("td.titleColumn") %>%
  html_nodes("a") %>%
  keep(html_text(.) == "The Shawshank Redemption") %>%
  html_attr("href")
```

But it is not the complete url, we need to base url.
```{r}
shawshank_full_url <- str_c("https://www.imdb.com/", shawshank_url)
```

Then we could futher scrape things from `shawshank_full_url`.

Besides using node class, you could also search a node by its `id`.


## Stackoverflow example

Here we are first extracting the `div` node with `id="questions"`.

```{r}
read_html("https://stackoverflow.com/questions/tagged/r") %>%
  html_node("div#questions") %>%
  html_nodes("div.summary") %>%
  html_nodes("h3") %>%
  html_nodes("a") %>%
  html_text()
```


# Scrapping dynamic webpages

`rvest` is only able to scrape static web pages. https://stats.nba.com/ is one of such websites that are not static.

PS: actually nba.com has a undocumented API, see https://github.com/seemethere/nba_py/wiki/stats.nba.com-Endpoint-Documentation

If you want to scrape dynamic web pages, you will need to control a browser programatically.

```{r}
library(RSelenium)
library(wdman)
```

```{r}
port <- httpuv::randomPort()
server <- chrome(port = port, version = "80.0.3987.106", verbose = FALSE)
rd <- remoteDriver(browserName = "chrome", port = port)
```

```{r}
rd$open(silent = TRUE)
rd$navigate("https://stats.nba.com/leaders/?SeasonType=Regular%20Season")
```

```{r}
rd$getPageSource() %>%
  str_flatten() %>%
  read_html() %>%
  html_node("div.nba-stat-table__overflow table") %>%
  html_table()
```

loop over the table by clicking the next button
```{r}
leader <- NULL
for (i in 1:6) {
  leader <- rd$getPageSource() %>%
    str_flatten() %>%
    read_html() %>%
    html_node("div.nba-stat-table__overflow table") %>%
    html_table() %>%
    bind_rows(leader, .)
  nextbutton <- rd$findElement("css", "a.stats-table-pagination__next")
  nextbutton$clickElement()
}
```

```{r}
leader
```

```{r}
# close the browser finally
server$stop()
```
